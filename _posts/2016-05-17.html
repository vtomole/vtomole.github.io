<!DOCTYPE html>
<html>
<head>
<title>Into The Methods of Solving a Markov Decision Process</title>
</head>
<body>

<h1>Into The Methods of Solving a Markov Decision Process</h1>
<p>
There are three methods of solving a Markov Decision process, these methods are, Dynamic Programming, Monte Carlo, and Temporal Difference Learning.</p>

<p>Dynamic Programming is a method where you know the full Markov Decision Process. The pros of this method, is that there is a specific algorithm that you can use to find your value function. The way this method is taught is “grid world” This is a grid that contains the value functions and determines the optimal policy that each state will contain until the state terminates. I am still trying to understand this grid world, so I can’t go into the specifics aspects of it.</p>

<p>Reinforcement learning depends on optimizing value functions. There also some situations where you don’t know what your Markov Decision Process is. This is an area that is suited for the Monte Carlo Method.  The Monte Carlo Method relies on random sampling and it is also used for numerical integration.  This random sampling is akin to the agent trying out everything in its environment and finding out what maximizes it’ s value function. This means that the Monte Carlo method learns from experience.</p>

<p>Temporal Difference Learning is a fundamental concept in reinforcement learning because it combines Dynamic Programming and the Monte Carlo Method. This method, like Monte Carlo also learns from experience, and it doesn’t have a Markov Decision Process. Unlike the Monte Carlo Method, Temporal Difference learning does not need the state to terminate to get the value function. Like Dynamic Programming, Temporal Difference Learning updates the value functions on the fly, the episode does not have to terminate. This is called bootstrapping.</p>

<p>The methods that solve a Markov Decision Process have the same steps. The first step is the policy evaluation that estimates the value function for a policy. The second step is finding the optimal policy, which is improving the policy when you have a value function. 
</p>

<p>References:</p>
Sutton, Richard S., and Andrew G. Barto. Reinforcement Learning: An Introduction. Cambridge, MA: MIT, 1998. Print. 


</body>
</html>
