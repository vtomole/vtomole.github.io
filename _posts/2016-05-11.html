<!DOCTYPE html>
<html>
<head>
<title>Into Markov Chains</title>
</head>
<body>

<h1>Into Markov Chains</h1>
<p>
The concept of Reinforcement Learning is thankfully not complicated at all. In my last post, I was wondering how I was going to implement a reward process for my agent. 
David Silver’s second Reinforcement Learning lecture answered that.
He said that all Reinforcement Learning problems can be conceptualized in the form of Markov Chains.
An awesome tutorial for Markov chains that I used is <a href ="http://setosa.io/ev/markov-chains/">here </a>.</p>


<p>A Markov Chain is a process of a state space (just a list of states) that can be drawn as a graph.
The process of a Markov Chain is based on a probability of a cursor moving from one state to another.
I am familiar with state spaces because I studied Finite State Machines last semester. For my class final project,
I created a state machine and I used it to implement a random number generator. 
The good thing about Markov Chains is that it does not need to remember all of the past events, it only uses that current event to decide the next state. 
This is called the Markov Property.</p 

<p>The Markov Chain can be modeled in 2 ways, with directed graphs; where vertices represent the state and the edges represent the probabilities, and a transition matrix.
The transition matrix is used more often because a Markov Chain can get very complex. 
These complexities can get so big that it may be impossible or tedious to model a Markov Chain using a graph.
This is why a transition matrix is almost always used to model Markov Chains.  "Every state in the state space is included once as a row and again as a column. And each cell in the matrix tells you the probability of transitioning from its row state to its column state."- Powell.
</p>
	
<p>I won’t get into the specifics of transition matrix mathematics (because I don't know any of it yet), but I will create transition matrix. 
This transition matrix models my day to day life in the summer. The main activities that I do in the summer is sleep, eat, study, and hang out online. 
I am going to use numbers between 0 and 1 to model the probabilities where a 10 percent chance is 0.10 and a 50 percent chance is .50. 
I created this transition matrix by first creating a state diagram, but I am unable to put a state diagram on here at the moment.
My state diagram starts from sleeping and it goes on from there. Every outgoing edge from any one of my states should add up to 1 because all probabilities are supposed to add up to 100 percent.</p>

 
<p>The transition matrix below conceptualizes a Markov Chain beautifully. It shows all of the transitions from a state to other states, for example the probability that I go from sleeping to sleeping again is 0.1, the probability that I go from eating to hanging out online is .75. The columns of the matrix are supposed to represent the arrows going from each state so each column should add up to 1 because there is 100 percent probability that something will happen no matter what state I am in.</p>


<table border="2">
<tr>
<td></td>
<td>Sleeping</td>
<td>Eating</td>
<td>Studying</td>
<td>Hanging out online</td>
</tr>
<tr>
<td>Sleeping</td>
<td>0.1</td>
<td>0.25</td>
<td>0.50</td>
<td>0.075</td>
</tr>

<tr>
<td>Eating</td>
<td>0.30</td>
<td>0.05</td>
<td>0.10</td>
<td>0.75</td>
</tr>
<tr>
<td>Studying</td>
<td>0.02</td>
<td>0.175</td>
<td>0.30</td>
<td>0.15</td>
</tr>
<tr>
<td>Hanging out online</td>
<td>0.67</td>
<td>0.75</td>
<td>0.10</td>
<td>0.70</td>
</tr>
</table>

<p>The next step is figuring out how to model the reward process using Markov Chains.</p>

<p>References:</p>
<p>Powell, Victor. "Markov Chains Explained Visually." Explained Visually. N.p., n.d. Web. 11 May 2016.</p>

<p>Silver, David. "Markov Decision Processes." SpringerReference (n.d.): n. pag. 2015. Web. 5 May 2016. </p> 


</p>

</body>
</html>